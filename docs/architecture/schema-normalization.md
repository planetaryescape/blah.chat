# Schema Normalization Reference Guide

**Purpose**: Preserve knowledge and decision-making context from the complete schema normalization migration (Phases 1-7)
**Audience**: Future maintainers, developers extending the system
**Status**: Migration complete (Phases 1-2), remaining phases documented for future reference

---

## Executive Summary

### What We Did

Transformed blah.chat's Convex database from a document-oriented, nested structure to a SQL-ready, normalized schema. This was a zero-downtime migration affecting messages, sources, attachments, tool calls, user preferences, tags, project relationships, and conversation metadata.

### Why We Did This

**Performance Crisis**:
- Message documents averaged 40% bloat from nested arrays
- Conversation deletion took 2+ seconds (O(N) project scans)
- Tag autocomplete impossible (4 separate case-sensitive arrays)
- No analytics capability (can't query "which model uses most tokens")

**Data Integrity Issues**:
- Custom instructions mutation destroyed new fields on save
- Monthly rebuild cron needed to fix project array drift
- Duplicate attachments on branched conversations
- No source metadata deduplication

### What We Achieved

**Completed (Phases 1-2)**:
- 40% message size reduction (attachments ‚Üí separate table)
- Foundation for 10x faster deletes (normalized tool calls)
- Source metadata deduplication (93.3% enrichment rate)
- Type safety improvements (native JSON, no string parsing)

**Designed (Phases 3-7)**:
- 10x faster cascade deletes (junction tables)
- Atomic preference updates (key-value store)
- Centralized tag management (autocomplete, consistency)
- Per-model token analytics (conversation usage tracking)
- N+1 query elimination (batch fetches, vector index)

---

## Architecture Overview

### Before: Document-Oriented (Denormalized)

```
messages: {
  _id: "msg1",
  content: "...",
  attachments: [                    // ‚ùå 40% bloat
    { type: "image", storageId, metadata: {...} }
  ],
  toolCalls: [...],                 // ‚ùå Duplicate state
  partialToolCalls: [...],          // ‚ùå Orphaned partials
  sources: [                        // ‚ùå No deduplication
    { id: "1", url, title, snippet }
  ],
  sourceMetadata: [                 // ‚ùå Repeated across messages
    { sourceId: "1", ogTitle, ogImage, ... }
  ]
}

projects: {
  conversationIds: [...]            // ‚ùå O(N) deletes, drift
}

users: {
  preferences: {                    // ‚ùå Destructive updates
    customInstructions: {...},
    theme: "...",
    // ... 30+ nested fields
  }
}

bookmarks/snippets/notes/feedback: {
  tags: ["Important", "important"] // ‚ùå Case-sensitive chaos
}
```

### After: Normalized (SQL-Ready)

```
messages: {
  _id, content, model, status, ...
}

attachments: {                      // ‚úÖ Phase 1
  messageId ‚Üí messages._id,
  conversationId, userId,
  storageId, type, metadata
}

toolCalls: {                        // ‚úÖ Phase 1
  messageId ‚Üí messages._id,
  toolCallId, args, result,
  isPartial (consolidates partial state)
}

sources: {                          // ‚úÖ Phase 2
  messageId ‚Üí messages._id,
  urlHash, position, provider
}

sourceMetadata: {                   // ‚úÖ Phase 2
  urlHash (PK),
  url, title, description,
  enriched, accessCount
}

projectConversations: {             // üìã Phase 3 (designed)
  projectId ‚Üí projects._id,
  conversationId ‚Üí conversations._id
}

userPreferences: {                  // üìã Phase 4 (designed)
  userId, category, key, value
}

tags: {                             // üìã Phase 5 (designed)
  name (normalized), displayName,
  usageCount
}

bookmarkTags/snippetTags/...: {    // üìã Phase 5 (designed)
  entityId, tagId ‚Üí tags._id
}

conversationTokenUsage: {           // üìã Phase 6 (designed)
  conversationId, model,
  totalTokens, inputTokens, outputTokens
}
```

---

## Implementation Patterns

### Pattern 1: Dual-Write Migration (Zero Downtime)

**Used in**: Phases 1-6

**Why**: Allows gradual rollout with instant rollback capability.

**6-Phase Approach** (Stripe pattern):
```
1. SCHEMA      ‚Üí Add new tables, keep old fields as optional
2. BACKFILL    ‚Üí Migrate existing data (idempotent, resumable)
3. DUAL-WRITE  ‚Üí Write to both old + new locations
4. DUAL-READ   ‚Üí Read from new first, fallback to old
5. CLEANUP     ‚Üí Remove old fields from data
6. COMPLETE    ‚Üí Remove old fields from schema
```

**Code Example** (Phase 1):
```typescript
// Step 3: Dual-write (write to both)
export const upsertToolCall = internalMutation({
  handler: async (ctx, args) => {
    // Write to new table (source of truth)
    await ctx.db.insert("toolCalls", {
      messageId: args.messageId,
      toolCallId: args.toolCallId,
      args: args.args,  // Native JSON (not stringified)
      isPartial: args.isPartial,
    });

    // ALSO write to old location (backward compat)
    const message = await ctx.db.get(args.messageId);
    const oldPartials = message?.partialToolCalls || [];
    await ctx.db.patch(args.messageId, {
      partialToolCalls: [...oldPartials, {
        id: args.toolCallId,
        arguments: JSON.stringify(args.args),  // Legacy format
      }],
    });
  },
});

// Step 4: Dual-read (read from new, fallback to old)
async function getMessageToolCalls(ctx, messageId) {
  // Try new table first
  const newToolCalls = await ctx.db
    .query("toolCalls")
    .withIndex("by_message", q => q.eq("messageId", messageId))
    .collect();

  if (newToolCalls.length > 0) {
    return newToolCalls;  // New source of truth
  }

  // Fallback to old structure (for unmigrated messages)
  const message = await ctx.db.get(messageId);
  return message?.toolCalls?.map(tc => ({
    toolCallId: tc.id,
    args: JSON.parse(tc.arguments),  // Parse legacy format
    isPartial: false,
  })) || [];
}
```

**Benefits**:
- Zero user-visible downtime
- Instant rollback (revert code, keep old fields)
- Gradual confidence building

**Gotchas**:
- Race conditions during concurrent updates (mitigate: Convex mutations are atomic)
- Dual-write adds latency (acceptable for migration period)
- Must maintain both code paths for 7-30 days

---

### Pattern 2: Cursor-Based Backfill (Resumable)

**Used in**: All phases

**Why**: Convex actions have 10min timeout. Cursor pagination enables resume.

**Code Example** (Phase 2):
```typescript
export const backfillSources = internalMutation({
  args: {
    cursor: v.optional(v.string()),
    batchSize: v.number()
  },
  handler: async (ctx, { cursor, batchSize }) => {
    // Paginate with cursor (not offset)
    const messages = await ctx.db
      .query("messages")
      .order("desc")
      .paginate({ cursor, numItems: batchSize });

    let sourcesCreated = 0;

    for (const msg of messages.page) {
      // Skip if already migrated (idempotent)
      if (!msg.sources?.length) continue;

      for (const src of msg.sources) {
        // Insert into new table
        await ctx.db.insert("sources", {
          messageId: msg._id,
          urlHash: generateHash(src.url),
          position: src.position,
        });
        sourcesCreated++;
      }
    }

    return {
      done: messages.isDone,
      nextCursor: messages.continueCursor,  // Resume point
      processed: messages.page.length,
      sourcesCreated,
    };
  },
});

// Orchestrator action (calls mutation in loop)
export const migrateMessageSources = internalAction({
  handler: async (ctx) => {
    let cursor: string | undefined;
    let totalProcessed = 0;

    do {
      const result = await ctx.runMutation(
        internal.migrations.backfillSources,
        { cursor, batchSize: 100 }
      );

      cursor = result.nextCursor;
      totalProcessed += result.processed;

      console.log(`‚úÖ Migrated ${totalProcessed} messages`);
    } while (cursor);  // Continue until no more pages
  },
});
```

**Benefits**:
- Handles millions of records
- Survives timeouts (re-run continues)
- Idempotent (safe to run multiple times)
- Progress tracking built-in

**Gotchas**:
- Don't use offset pagination (doesn't scale)
- Batch size affects memory (100-1000 typical)
- Cursor invalidated if table modified during migration (rare)

---

### Pattern 3: Hash-Based Deduplication

**Used in**: Phases 2, 5

**Why**: Normalize shared data (source metadata, tags) across entities.

**Code Example** (Phase 2):
```typescript
// Generate stable ID from URL
function generateSourceId(url: string): string {
  return crypto
    .createHash("sha256")
    .update(url)
    .digest("hex")
    .slice(0, 16);  // 16 chars sufficient
}

// Insert source with metadata deduplication
export const addSources = internalMutation({
  handler: async (ctx, args) => {
    for (const src of args.sources) {
      const urlHash = generateSourceId(src.url);

      // Insert source record (per-message)
      await ctx.db.insert("sources", {
        messageId: args.messageId,
        urlHash,
        position: src.position,
      });

      // Check if metadata already exists
      const existingMetadata = await ctx.db
        .query("sourceMetadata")
        .withIndex("by_urlHash", q => q.eq("urlHash", urlHash))
        .unique();

      if (!existingMetadata) {
        // Create metadata (shared across messages)
        await ctx.db.insert("sourceMetadata", {
          urlHash,
          url: src.url,
          accessCount: 1,
          enriched: false,
        });

        // Schedule async enrichment
        await ctx.scheduler.runAfter(
          0,
          internal.sources.enrichSourceMetadata,
          { urlHash, url: src.url }
        );
      } else {
        // Increment access count (deduplication metric)
        await ctx.db.patch(existingMetadata._id, {
          accessCount: existingMetadata.accessCount + 1,
          lastAccessedAt: Date.now(),
        });
      }
    }
  },
});
```

**Benefits**:
- 30-50% storage savings (typical deduplication ratio)
- Single source of truth for shared metadata
- Enables analytics (most-cited sources, tag trends)

**Gotchas**:
- Hash collisions theoretically possible (use SHA-256, not MD5)
- Case sensitivity matters (normalize before hashing)
- URL variants ("http" vs "https", trailing slash) should be normalized

---

### Pattern 4: TypeScript Type Depth Workaround

**Used in**: All phases (94+ Convex modules hit recursion limits)

**Why**: TypeScript can't resolve `internal.*` and `api.*` types at this scale.

**Official Convex Recommendation**:
Extract 90% of logic to plain TypeScript helpers, keep query/mutation/action wrappers thin (10%).

**Pragmatic Workaround** (when helpers not feasible):
```typescript
// Backend (Convex actions) - Complex cast
const result = ((await (ctx.runQuery as any)(
  // @ts-ignore - TypeScript recursion limit with 94+ Convex modules
  internal.path.to.query,
  { args },
)) as ReturnType);

// Frontend (React hooks) - Direct @ts-ignore
// @ts-ignore - Type depth exceeded with complex Convex mutation
const myMutation = useMutation(api.path.to.mutation);
```

**Benefits**:
- Full type safety on return values
- Only bypasses parameter type inference
- Explicit return types make code self-documenting

**Locations** (from actual codebase):
- `convex/transcription.ts` - getCurrentUser, recordTranscription
- `convex/search/hybrid.ts` - fullTextSearch, vectorSearch
- `convex/ai/generateTitle.ts` - getConversationMessages

**Alternative Considered**: Casting function signature ‚Üí Still causes recursion before application.

---

## Phase-by-Phase Learnings

### Phase 1: Message Attachments & Tool Calls (‚úÖ COMPLETE)

**Timeline**: 10 days (Dec 2024)
**Scope**: Extract nested `attachments[]`, `toolCalls[]`, `partialToolCalls[]` to tables

#### Key Decisions

**1. Consolidate Partial State with Boolean Flag**

*Before*:
```typescript
messages: {
  toolCalls: [...],         // Complete tool calls
  partialToolCalls: [...]   // Streaming state
}
```

*After*:
```typescript
toolCalls: {
  isPartial: boolean  // Single table, one flag
}
```

**Why**: Eliminates orphaned partial state bugs. No cleanup needed when transitioning from partial ‚Üí complete.

**2. Native JSON Storage (Not Stringified)**

*Before*:
```typescript
toolCalls: [{
  arguments: JSON.stringify(args),  // String
  result: JSON.stringify(output)    // String
}]
```

*After*:
```typescript
toolCalls: {
  args: v.any(),    // Native JSON
  result: v.any()   // Native JSON
}
```

**Why**: AI SDK v5 pattern. No serialization overhead. Type-safe queries. Eliminates `JSON.parse` errors.

**3. User Scoping on All Tables**

```typescript
attachments/toolCalls: {
  userId: v.id("users")  // Not just messageId
}
```

**Why**: Enables future row-level security, per-user analytics, multi-tenant queries.

#### Implementation Results

- **Migrated**: 834 messages in 6.13s (136 msg/sec)
- **Extracted**: 14 attachments, 62 tool calls
- **Cleaned**: 55 messages (deprecated fields removed)
- **Performance**: 40% message size reduction

#### Critical Gotchas Encountered

**1. Attachment Streaming Doesn't Exist**

*Discovery*: Attachments added via `addAttachment` mutation AFTER generation completes. No partial attachments needed.

*Impact*: Simplified migration (no streaming state to handle).

**2. Tool Call textPosition Critical for UI**

*Discovery*: `textPosition` field determines where inline tool display appears in content.

*Code*:
```typescript
// generation.ts:744
textPosition: accumulated.length  // Character position in stream
```

*Gotcha*: Must preserve during migration. If missing, shows all tool calls at top (legacy behavior).

**3. Tool Result JSON Double-Encoding Risk**

*Before*:
```typescript
// Old: Stringified storage
toolCall.result = JSON.stringify(resultValue);

// Frontend: Parse
const parsed = JSON.parse(toolCall.result);
```

*After*:
```typescript
// New: Native JSON
toolCall.result = resultValue;  // v.any()

// Frontend: Direct access
const parsed = toolCall.result;  // No parse needed
```

*Fix Required*: Update frontend to NOT parse, or migration creates double-encoded strings.

**4. Partial Tool Calls Must Clear on Completion**

*Old behavior*:
```typescript
await ctx.db.patch(messageId, {
  toolCalls: finalToolCalls,
  partialToolCalls: undefined,  // ‚ö†Ô∏è Critical: Clear streaming state
});
```

*New behavior*:
```typescript
// Mark all partial tool calls as complete
const partials = await ctx.db
  .query("toolCalls")
  .withIndex("by_message_partial", q =>
    q.eq("messageId", messageId).eq("isPartial", true)
  )
  .collect();

for (const tc of partials) {
  await ctx.db.patch(tc._id, { isPartial: false });
}
```

*Why*: Prevents stuck loading states. UI filters by `isPartial` flag.

#### Testing Insights

**Resilient Generation Test** (CRITICAL):
1. Start message with tool calls
2. Refresh browser mid-stream
3. Expected: Generation continues server-side, tool calls complete

*Result*: ‚úÖ Passed. Tool calls survived refresh (server-side Convex action persistence).

**Vision Model Filtering**:
- Only vision models process attachments
- Non-vision models silently ignore attachments
- Don't show loading state for attachments on non-vision models

---

### Phase 2: Message Sources & Metadata (‚úÖ COMPLETE)

**Timeline**: 8 days (Dec 2024)
**Scope**: Extract `sources[]`, `sourceMetadata[]`, deduplicate by URL

#### Key Decisions

**1. Rename Fields for Clarity**

| Doc | Implementation | Reason |
|-----|----------------|--------|
| `sourceId` | `urlHash` | More descriptive |
| `fetchCount` | `accessCount` | Clearer meaning |
| `error` | `enrichmentError` | Specific to enrichment |
| `fetchedAt` | `enrichedAt` | Matches lifecycle |

**2. Track Enrichment Lifecycle**

```typescript
sourceMetadata: {
  enriched: boolean,
  enrichedAt?: number,
  enrichmentError?: string,
  firstSeenAt: number,
  lastAccessedAt: number,
}
```

**Why**: Debugging. Can query "which metadata failed enrichment" or "stale metadata needing refresh".

**3. Provider Tracking**

```typescript
sources: {
  provider: "perplexity" | "openrouter" | "webSearch" | "generic"
}
```

**Why**: Analytics. Can answer "which provider's sources are most cited".

#### Implementation Results

- **Migrated**: 15 sources, 15 sourceMetadata in <1s
- **Deduplication**: 0% (only 1 message with sources in test DB)
- **Enrichment**: 93.3% (14/15 metadata enriched within 5s)
- **Data integrity**: 100% (all samples matched legacy count)

#### Critical Gotchas Encountered

**1. Source ID is Ephemeral in Old Schema**

*Old format*:
```typescript
sources: [
  { id: "1", url: "..." },  // Sequential position
  { id: "2", url: "..." },
]
```

*New format*:
```typescript
sources: {
  position: 1,              // Display order
  urlHash: "a3f2e1..."      // Stable ID
}
```

*Gotcha*: Citation markers `[1]`, `[2]` in content won't break because we store `position` field separately.

**2. Provider Metadata Format Inconsistency**

*3 different extraction patterns found*:
```typescript
// OpenRouter
providerMetadata.openrouter.search_results[]

// Perplexity
providerMetadata.perplexity.citations[]

// Generic
providerMetadata.citations[]
```

*Solution*: Handle all 3 during extraction (generation.ts:271-364). Unified into single `sources` table with `provider` field.

**3. OpenGraph Enrichment Runs Async**

*Flow*:
```
Source created ‚Üí Schedule enrichment ‚Üí 0-5s delay ‚Üí Metadata updated
```

*UI Impact*: Sources visible before metadata loads. Must handle:
```typescript
// Fallback to basic data if enrichment incomplete
title: source.metadata?.title || source.title
favicon: source.metadata?.favicon || defaultFavicon
```

**4. URL Deduplication Edge Case**

*Scenario*: Same URL cited in messages A and B, but different titles (provider variance).

*Solution*:
- `sources` table: Stores per-message title (as provided by model)
- `sourceMetadata` table: Stores canonical OpenGraph title
- UI: Prefers `metadata.title` over `source.title` for consistency

**5. Partial Sources Not Implemented**

*Schema has* `isPartial: boolean` *field*.

*Current usage*: Always `false` (streaming not implemented for sources).

*Why keep*: Future-proofing. If streaming citations added, field ready.

#### Testing Insights

**Enrichment Success Rate**: 93.3% achieved in production.

*Failures from*:
- Paywall sites (403 errors)
- Invalid URLs (malformed)
- Sites blocking OpenGraph scraping

*Not a bug*: Enrichment is best-effort. Fallback to basic data (title, URL) always works.

**Citation Marker Preservation**:
- Citations `[1]`, `[2]` in content string not changed
- `position` field determines display order in source list
- Click handler uses `id="source-${position}"` for smooth scroll

---

### Phase 3: Project Relationships (üìã DESIGNED)

**Timeline**: 6-8 days (planned)
**Scope**: Replace `conversationIds[]` array with junction table

#### The Cascade Delete Disaster (Why This Phase Matters)

**Current O(P √ó C) complexity**:
```typescript
// Delete conversation ‚Üí scan ALL user's projects
const projects = await ctx.db
  .query("projects")
  .withIndex("by_user", q => q.eq("userId", user._id))
  .collect();  // ‚ö†Ô∏è Fetches ALL (20+ projects)

for (const project of projects) {
  if (project.conversationIds.includes(convId)) {  // ‚ö†Ô∏è O(C) per project
    await ctx.db.patch(project._id, {
      conversationIds: project.conversationIds.filter(
        id => id !== convId  // ‚ö†Ô∏è Full array rebuild
      ),
    });
  }
}
```

**Performance**:
- User with 20 projects √ó 100 conversations each = 2,000 array scans
- Bulk delete 50 conversations = 100,000 operations
- Each patch rewrites entire array (8KB for 1000 conversations)

**After O(1) indexed lookup**:
```typescript
// Direct index lookup - only fetches projects containing this conversation
const links = await ctx.db
  .query("projectConversations")
  .withIndex("by_conversation", q => q.eq("conversationId", convId))
  .collect();  // Typically 0-1 results

// Delete links (fast)
for (const link of links) {
  await ctx.db.delete(link._id);
}
```

**Performance**: 2000 ops ‚Üí 1-2 ops (10-100x faster)

#### The Stale Array Problem

**Why monthly rebuild cron exists**:
```typescript
// Failure scenario:
// Thread 1: Delete conversation
await ctx.db.patch(project._id, {
  conversationIds: project.conversationIds.filter(...)  // Array cleaned
});
// ‚ö†Ô∏è CRASH before reaching conversation delete
await ctx.db.delete(conversationId);  // Never executes

// Result: Array cleaned but conversation still exists
// UI shows wrong count until monthly rebuild
```

**Solution**: Single source of truth (junction table) eliminates drift.

#### Schema Design

```typescript
projectConversations: defineTable({
  projectId: v.id("projects"),
  conversationId: v.id("conversations"),
  addedAt: v.number(),
  addedBy: v.id("users"),  // Audit trail
})
  .index("by_project", ["projectId"])
  .index("by_conversation", ["conversationId"])
  .index("by_project_conversation", ["projectId", "conversationId"]);
```

#### Critical Gotcha: Dual Source of Truth During Transition

**Risk**: Array and junction table get out of sync during dual-write phase.

**Mitigation Strategy**:
1. ALWAYS update `conversation.projectId` first (source of truth)
2. Then update junction table
3. Finally update array (cache)
4. If crash occurs, monthly rebuild fixes array (temporary safety net)

**After cleanup**: Junction table only. No monthly rebuild needed.

---

### Phase 4: User Preferences (üìã DESIGNED)

**Timeline**: 8-10 days (planned)
**Scope**: Flatten `preferences: v.object({...})` to key-value table

#### The Custom Instructions Mutation Bug

**File**: `convex/users.ts:226-240`

**Buggy code**:
```typescript
// ‚ùå DESTRUCTIVE: Reconstructs entire object
customInstructions: {
  aboutUser: args.aboutUser,
  responseStyle: args.responseStyle,
  enabled: args.enabled,
  baseStyleAndTone: args.baseStyleAndTone,
  nickname: args.nickname,
  occupation: args.occupation,
  moreAboutYou: args.moreAboutYou,
},
```

**Problem**: If Phase 5 adds new field `customInstructions.timezone`, this mutation **loses it** on save.

**Contrast with safe mutation** (`updatePreferences`, lines 169-175):
```typescript
// ‚úÖ SAFE: Spread merge preserves unknown fields
preferences: {
  ...user.preferences,
  ...args.preferences,
},
```

**Solution**: Key-value store enables atomic updates.
```typescript
// Update only one preference
await updateUserPreference({
  key: "aboutUser",
  value: "Software engineer interested in AI"
});

// Other preferences untouched
```

#### Schema Design

```typescript
userPreferences: defineTable({
  userId: v.id("users"),
  category: v.union(
    v.literal("appearance"),
    v.literal("models"),
    v.literal("chat"),
    v.literal("audio"),
    v.literal("advanced"),
    v.literal("customInstructions")
  ),
  key: v.string(),
  value: v.any(),  // JSON value (string, number, boolean, object)
  updatedAt: v.number(),
  createdAt: v.number(),
})
  .index("by_user", ["userId"])
  .index("by_user_category", ["userId", "category"])
  .index("by_user_key", ["userId", "key"]);
```

#### Nested Object vs Individual Keys Trade-off

**Option A: Store customInstructions as nested object**
```typescript
{ key: "customInstructions.all", value: { aboutUser: "...", ... } }
```
**Pros**: Backward compatible, single query
**Cons**: Still nested, harder to query individual fields

**Option B: Store as individual keys**
```typescript
{ key: "aboutUser", category: "customInstructions", value: "..." }
{ key: "responseStyle", category: "customInstructions", value: "..." }
```
**Pros**: Fully flat, queryable, atomic updates
**Cons**: More rows, more queries

**Recommendation**: Start with Option A (nested object for complex prefs like customInstructions), Option B for simple prefs (theme, sendOnEnter). Can flatten further later.

#### Critical Gotcha: Preference Defaults Scattered Everywhere

**Current**: Defaults in component state initialization
```typescript
// UISettings.tsx:54
setShowActions(user.preferences.alwaysShowMessageActions ?? false);  // default: false

// generation.ts:175
const showByDefault = user.preferences.reasoning?.showByDefault ?? true;  // default: true
```

**Problem**: No single source of truth for default values. Different files have different defaults for same field.

**Solution**: Centralize defaults
```typescript
// convex/users.ts
export const PREFERENCE_DEFAULTS = {
  theme: "dark",
  sendOnEnter: true,
  alwaysShowMessageActions: false,
  showMessageStatistics: true,
  // ...
};

export const getUserPreferenceWithDefault = query({
  args: { key: v.string() },
  handler: async (ctx, { key }) => {
    const userId = await getCurrentUserId(ctx);
    const pref = await ctx.db
      .query("userPreferences")
      .withIndex("by_user_key", q => q.eq("userId", userId).eq("key", key))
      .unique();

    return pref?.value ?? PREFERENCE_DEFAULTS[key];  // Single source of truth
  },
});
```

---

### Phase 5: Centralized Tags (üìã DESIGNED)

**Timeline**: 10-12 days (planned)
**Scope**: Unify tags across bookmarks, snippets, notes, feedback

#### The Case-Sensitivity Nightmare

**Current chaos**:
```typescript
// User creates all of these as separate tags:
bookmarks: { tags: ["Important"] }
snippets: { tags: ["important"] }
notes: { tags: ["IMPORTANT"] }
feedback: { tags: [" Important "] }  // Whitespace variant
```

**Result**: 4 "different" tags for same concept. No autocomplete possible.

**After normalization**:
```typescript
tags: {
  name: "important",           // Normalized (lowercase, trimmed)
  displayName: "Important",    // Original casing (first occurrence)
  usageCount: 4
}

bookmarkTags: { bookmarkId, tagId }
snippetTags: { snippetId, tagId }
noteTags: { noteId, tagId }
feedbackTags: { feedbackId, tagId }
```

**Benefits**:
- Autocomplete: `searchTags("imp")` ‚Üí "important" (4 uses)
- Consistency: One canonical tag
- Rename cascades: `renameTag("important", "urgent")` updates all entities

#### Tag Rename Doesn't Cascade (Current Bug)

**Notes only** (convex/notes.ts) has `renameTag` mutation.
**Bookmarks/snippets/feedback**: No rename capability.

**Scenario**:
1. User renames `"project-a"` ‚Üí `"proj-a"` in notes
2. Bookmarks still have `"project-a"` (orphaned)
3. Tag autocomplete shows both variants

**Solution**: Central `renameTag` cascades to all entities
```typescript
export const renameTag = mutation({
  args: { oldName: v.string(), newName: v.string() },
  handler: async (ctx, { oldName, newName }) => {
    const oldTag = await ctx.db
      .query("tags")
      .withIndex("by_name", q => q.eq("name", oldName.toLowerCase()))
      .unique();

    // Update tag (cascades to all entities via foreign key)
    await ctx.db.patch(oldTag._id, {
      name: newName.toLowerCase(),
      displayName: newName,
    });
  },
});
```

#### Schema Design

```typescript
tags: defineTable({
  name: v.string(),          // Normalized (lowercase, trimmed)
  displayName: v.string(),   // Original casing
  color: v.optional(v.string()),
  usageCount: v.number(),    // Denormalized for sorting
  category: v.optional(v.union(
    v.literal("bookmark"),
    v.literal("snippet"),
    v.literal("note"),
    v.literal("feedback"),
    v.literal("general")
  )),
  createdAt: v.number(),
  createdBy: v.id("users"),
})
  .index("by_name", ["name"])
  .index("by_usage", ["usageCount"])
  .index("by_category", ["category", "usageCount"]);

// One junction table per entity
bookmarkTags: defineTable({
  bookmarkId: v.id("bookmarks"),
  tagId: v.id("tags"),
  addedAt: v.number(),
  addedBy: v.id("users"),
})
  .index("by_bookmark", ["bookmarkId"])
  .index("by_tag", ["tagId"]);
```

#### Critical Gotcha: Case-Sensitivity During Backfill

**Before migration**: `"Important"` ‚â† `"important"` (separate arrays)
**After migration**: Both map to same tag (`name: "important"`, `displayName` preserves casing)

**Problem**: During backfill, first occurrence wins for `displayName`. Users who typed `"Important"` might see `"important"` if that was inserted first.

**Solution Options**:
1. Use most common casing (max occurrence)
2. Use alphabetically first (consistent but arbitrary)
3. Preserve user's own casing per-entity (complex)

**Recommendation**: Option 1 (most common casing). Query all variants, pick most frequent.

---

### Phase 6: Conversation Metadata (üìã DESIGNED)

**Timeline**: 6-8 days (planned)
**Scope**: Extract `tokenUsage: v.object({...})` to per-model table

#### Why Per-Model Tracking Matters

**Current**: Single `tokenUsage` object on conversation
```typescript
conversation: {
  tokenUsage: {
    systemTokens: 500,
    messagesTokens: 12000,
    memoriesTokens: 300,
    totalTokens: 12800,
    contextLimit: 128000,
    lastCalculatedAt: Date.now(),
  }
}
```

**Problem**: Can't answer "which model used most tokens" when switching mid-conversation.

**After**: Per-model breakdown
```typescript
conversationTokenUsage: {
  conversationId: "conv1",
  model: "openai:gpt-4o",
  totalTokens: 8000,
  inputTokens: 6000,
  outputTokens: 2000,
  messageCount: 5
}

conversationTokenUsage: {
  conversationId: "conv1",
  model: "anthropic:claude-3-5-sonnet",
  totalTokens: 4800,
  inputTokens: 3500,
  outputTokens: 1300,
  reasoningTokens: 200,  // o1/o3 models only
  messageCount: 3
}
```

**Enables**:
- "Show me token usage by model for this conversation"
- "Which model is most cost-effective for this user?"
- "Track reasoning token usage separately"

#### Source of Truth: Messages, Not Cached Usage

**Current**: `tokenUsage` object manually updated, can drift from reality.

**New**: Always rebuildable from messages table.

**Gotcha**: If migration finds discrepancies between old `tokenUsage` and message totals, messages win.

**Verification Query**:
```typescript
// Check if cached usage matches actual sum
const messages = await ctx.db
  .query("messages")
  .withIndex("by_conversation", q => q.eq("conversationId", convId))
  .collect();

const actualTotal = messages.reduce((sum, msg) =>
  sum + (msg.inputTokens || 0) + (msg.outputTokens || 0), 0
);

const cachedTotal = conversation.tokenUsage?.totalTokens || 0;

if (actualTotal !== cachedTotal) {
  console.warn("Token usage drift detected!", { actualTotal, cachedTotal });
}
```

#### Critical Gotcha: Message Deletion Requires Decrement

**Current**: Deleting message doesn't update `tokenUsage` (drift).

**New**: Must decrement usage when message deleted.

```typescript
// In deleteMessage mutation:
if (message.inputTokens || message.outputTokens) {
  const usage = await ctx.db
    .query("conversationTokenUsage")
    .withIndex("by_conversation_model", q =>
      q.eq("conversationId", message.conversationId)
       .eq("model", message.model)
    )
    .unique();

  if (usage) {
    await ctx.db.patch(usage._id, {
      totalTokens: usage.totalTokens - (message.inputTokens + message.outputTokens),
      inputTokens: usage.inputTokens - message.inputTokens,
      outputTokens: usage.outputTokens - message.outputTokens,
      messageCount: usage.messageCount - 1,
    });
  }
}
```

---

### Phase 7: Final Optimizations (üìã DESIGNED)

**Timeline**: 8-10 days (planned)
**Scope**: Eliminate N+1 queries, optimize indexes, final polish

#### Optimization 1: Bookmarks N+1 Query

**Current O(N √ó 2) pattern**:
```typescript
const bookmarks = await ctx.db.query("bookmarks").collect();  // N bookmarks

const bookmarksWithData = await Promise.all(
  bookmarks.map(async (bookmark) => {
    const message = await ctx.db.get(bookmark.messageId);  // N queries
    const conversation = message
      ? await ctx.db.get(message.conversationId)  // N more queries
      : null;
    return { ...bookmark, message, conversation };
  }),
);
```

**Performance**: 10 bookmarks = 20 DB queries

**After batch fetch**:
```typescript
const bookmarks = await ctx.db.query("bookmarks").collect();  // 1 query

// Batch fetch messages
const messageIds = bookmarks.map(b => b.messageId);
const messages = await Promise.all(
  messageIds.map(id => ctx.db.get(id))
);  // 1 batch

// Batch fetch conversations (deduplicated)
const conversationIds = [
  ...new Set(messages.filter(Boolean).map(m => m!.conversationId))
];
const conversations = await Promise.all(
  conversationIds.map(id => ctx.db.get(id))
);  // 1 batch

// Join in memory
const conversationMap = new Map(
  conversations.filter(Boolean).map(c => [c!._id, c!])
);

return bookmarks.map((bookmark, i) => ({
  ...bookmark,
  message: messages[i],
  conversation: messages[i]
    ? conversationMap.get(messages[i]!.conversationId)
    : null,
}));
```

**Performance**: 10 bookmarks = 3 queries (7x faster)

#### Optimization 2: Use Native Vector Index

**Current manual scoring**:
```typescript
// ‚ö†Ô∏è Fetches ALL messages
const results = await ctx.db
  .query("messages")
  .withIndex("by_user", q => q.eq("userId", userId))
  .filter(q => q.neq(q.field("embedding"), undefined))
  .collect();  // 1000+ messages

// Computes cosine similarity client-side
const withScores = results.map((msg) => ({
  ...msg,
  score: cosineSimilarity(queryEmbedding, msg.embedding)
}));
```

**Problem**: For 1000 messages, computes 1000 cosine similarities in V8 runtime (~500ms).

**After native vector index**:
```typescript
// Use Convex vector index (already in schema!)
const results = await ctx.db
  .query("messages")
  .withIndex("by_embedding", q =>
    q.similar("embedding", queryEmbedding, limit * 2)  // Native similarity
     .eq("userId", userId)
  )
  .collect();  // <50ms
```

**Performance**: 500ms ‚Üí 50ms (10x faster)

**Gotcha**: Vector index filter fields limited to what's defined in schema:
```typescript
.vectorIndex("by_embedding", {
  vectorField: "embedding",
  filterFields: ["conversationId", "userId"],  // Can only filter by these
})
```

To filter by `role` (user vs assistant), must filter results in memory after query.

#### Optimization 3: Add Missing Indexes

**Index audit** (common queries without optimal indexes):
```typescript
// BEFORE: Full table scan
const recentBookmarks = await ctx.db
  .query("bookmarks")
  .withIndex("by_user", q => q.eq("userId", userId))
  .order("desc")  // ‚ö†Ô∏è Order by createdAt (not in index)
  .collect();

// AFTER: Compound index
bookmarks: defineTable({...})
  .index("by_user_created", ["userId", "createdAt"]);  // ‚úÖ ADD

const recentBookmarks = await ctx.db
  .query("bookmarks")
  .withIndex("by_user_created", q => q.eq("userId", userId))
  .order("desc")  // Uses index now
  .collect();
```

**Indexes to add**:
- `messages.by_conversation_created` - Ordered message lists
- `messages.by_conversation_role` - Filter user/assistant messages
- `memories.by_conversation` - Missing!
- `memories.by_user_category` - Filtered searches
- `feedback.by_status_priority` - Filtered admin views

**Gotcha**: Index build time for large tables. Deploy during low-traffic periods. Monitor Convex dashboard ‚Üí Indexes tab ‚Üí "Building" status.

---

## Common Patterns & Gotchas

### Gotcha 1: Data Cleanup Required Before Schema Changes

**Problem**: Can't remove schema field while data exists.

**Error**:
```
Schema validation failed: Field 'attachments' exists in documents but not in schema
```

**Solution**: Two-step process
```typescript
// Step 1: Cleanup migration (remove data from all documents)
export const cleanupDeprecatedFields = internalMutation({
  handler: async (ctx) => {
    const messages = await ctx.db.query("messages").collect();

    for (const msg of messages) {
      if (msg.attachments || msg.toolCalls) {
        await ctx.db.patch(msg._id, {
          attachments: undefined,
          toolCalls: undefined,
        });
      }
    }
  },
});

// Step 2: Schema deployment (remove field definition)
// Delete from schema.ts:
// attachments: v.optional(v.array(...))
```

### Gotcha 2: Migration State Tracking Essential

**Why**: Resumability prevents data loss if action times out.

**Implementation** (Stripe 6-phase pattern):
```typescript
migrations: defineTable({
  migrationId: v.string(),
  name: v.string(),
  phase: v.union(
    v.literal("schema"),
    v.literal("backfill"),
    v.literal("dual-write"),
    v.literal("dual-read"),
    v.literal("cleanup"),
    v.literal("complete")
  ),
  status: v.union(
    v.literal("pending"),
    v.literal("running"),
    v.literal("completed"),
    v.literal("failed"),
    v.literal("rolled-back")
  ),
  checkpoint: v.object({
    cursor: v.optional(v.string()),
    processedCount: v.number(),
    successCount: v.number(),
    errorCount: v.number(),
    lastProcessedId: v.optional(v.string()),
  }),
  startedAt: v.optional(v.number()),
  completedAt: v.optional(v.number()),
})
```

**Usage**:
```typescript
// Initialize migration
await ctx.db.insert("migrations", {
  migrationId: "001_normalize_attachments",
  name: "Normalize message attachments",
  phase: "backfill",
  status: "running",
  checkpoint: { processedCount: 0, successCount: 0, errorCount: 0 },
  startedAt: Date.now(),
});

// Update checkpoint after each batch
await ctx.db.patch(migrationRecord._id, {
  checkpoint: {
    cursor: result.nextCursor,
    processedCount: totalProcessed,
    successCount: totalSuccess,
    errorCount: totalErrors,
    lastProcessedId: lastId,
  },
});

// Mark complete
await ctx.db.patch(migrationRecord._id, {
  status: "completed",
  completedAt: Date.now(),
});
```

**Benefits**:
- Resume from checkpoint if action times out
- Audit trail (when did migration run, how long, any errors)
- Idempotent (check status before re-running)

### Gotcha 3: Convex Mutations Are Atomic (But Actions Aren't)

**Mutations** (V8 runtime):
- Atomic transactions
- If error thrown, ALL changes rolled back
- Safe from race conditions within mutation

**Actions** (Node runtime):
- NOT atomic
- Can call multiple mutations sequentially
- Race conditions possible between mutations

**Example**:
```typescript
// Thread 1: Assign conversation to projectA
await ctx.runMutation(api.projects.assignConversation, {
  projectId: projectA,
  conversationId: conv1
});
// Sets: conv1.projectId = projectA

// Thread 2: Assign conversation to projectB (concurrent!)
await ctx.runMutation(api.projects.assignConversation, {
  projectId: projectB,
  conversationId: conv1
});
// Sets: conv1.projectId = projectB (wins)

// Thread 1 continues:
await ctx.runMutation(api.projects.createLink, {
  projectId: projectA,
  conversationId: conv1
});
// Creates link to projectA

// Result: conv1.projectId = projectB but link exists to projectA
```

**Mitigation**: Validate before creating link
```typescript
// Before creating link, verify projectId matches
const conv = await ctx.db.get(conversationId);
if (conv.projectId !== projectId) {
  throw new Error("Conversation projectId changed during operation");
}
await ctx.db.insert("projectConversations", { projectId, conversationId });
```

### Gotcha 4: Batch Fetch Order Preservation

**Promise.all preserves order**, but `ctx.db.get()` for deleted entities returns `null`.

**Pattern**:
```typescript
const messageIds = bookmarks.map(b => b.messageId);
const messages = await Promise.all(ids.map(id => ctx.db.get(id)));

// messages[i] may be null if message was deleted
// MUST handle nulls gracefully
return bookmarks.map((bookmark, i) => ({
  ...bookmark,
  message: messages[i] || null,  // Explicit null handling
}));
```

**Don't do**:
```typescript
// ‚ùå Filter out nulls - breaks index alignment
const messages = (await Promise.all(
  ids.map(id => ctx.db.get(id))
)).filter(Boolean);

// Now messages.length !== bookmarks.length ‚Üí index mismatch
```

### Gotcha 5: Index Build Time for Large Tables

**Problem**: Adding index to table with 10,000+ rows takes minutes.

**Convex behavior**:
1. Schema deployed ‚Üí Index marked "Building"
2. Background job builds index (doesn't block queries)
3. Queries slow until index built (full table scan)

**Monitor**: Convex dashboard ‚Üí Indexes tab ‚Üí Check "Building" status

**Best practice**: Deploy indexes during low-traffic periods (e.g., 2am UTC).

**Workaround**: If urgent, use `.filter()` after `.collect()` instead of waiting for index:
```typescript
// Temporary workaround until index built
const allMessages = await ctx.db.query("messages").collect();
const filtered = allMessages.filter(m => m.conversationId === convId);

// After index built:
const filtered = await ctx.db
  .query("messages")
  .withIndex("by_conversation", q => q.eq("conversationId", convId))
  .collect();
```

---

## Future Enhancement Guide

### Adding New Tables

**Follow this checklist**:

1. **Schema design** (consider SQL normalization)
   - Primary key (always `_id: Id<"table">`)
   - Foreign keys (`parentId: v.id("parent")`)
   - Denormalized fields for filtering (e.g., `userId`)
   - Created/updated timestamps
   - Soft delete flag (optional: `deletedAt?: number`)

2. **Indexes** (query patterns first)
   - `by_<foreignKey>` for lookups (e.g., `by_user`)
   - Compound indexes for sorted queries (e.g., `by_user_created`)
   - Unique constraints where needed

3. **Migration plan**
   - Will data be migrated from existing structure?
   - If yes: Use dual-write pattern (6 phases)
   - If no: Just add table, write new data only

4. **Type safety**
   - Update `_generated/dataModel.d.ts` (auto-generated by Convex)
   - Add TypeScript interfaces in `src/types/`
   - Use `Doc<"table">` for document types, `Id<"table">` for IDs

5. **Queries/mutations**
   - Create CRUD operations in `convex/<table>.ts`
   - Export for frontend: `export const get... = query({...})`
   - Internal helpers: `export const internal... = internalQuery({...})`

### Adding New Relationships

**M:1 (Many-to-One)**: Add foreign key to child table
```typescript
comments: defineTable({
  postId: v.id("posts"),  // Foreign key
  // ...
})
  .index("by_post", ["postId"]);
```

**M:N (Many-to-Many)**: Create junction table
```typescript
postTags: defineTable({
  postId: v.id("posts"),
  tagId: v.id("tags"),
  // ...
})
  .index("by_post", ["postId"])
  .index("by_tag", ["tagId"]);
```

**1:1 (One-to-One)**: Foreign key with uniqueness constraint
```typescript
userProfiles: defineTable({
  userId: v.id("users"),  // Unique
  // ...
})
  .index("by_user", ["userId"]);

// Enforce uniqueness in mutation:
const existing = await ctx.db
  .query("userProfiles")
  .withIndex("by_user", q => q.eq("userId", userId))
  .unique();

if (existing) {
  throw new Error("Profile already exists");
}
```

### Optimizing Existing Queries

**Step 1**: Identify slow queries (Convex dashboard ‚Üí Functions ‚Üí p95 latency)

**Step 2**: Check if index used
```typescript
// In query code, check .withIndex() usage
const results = await ctx.db
  .query("messages")
  .withIndex("by_conversation", q => q.eq("conversationId", convId))  // ‚úÖ Index used
  .collect();

// vs

const results = await ctx.db
  .query("messages")
  .collect()  // ‚ùå Full table scan
  .filter(m => m.conversationId === convId);
```

**Step 3**: Add missing indexes
```typescript
// schema.ts
messages: defineTable({...})
  .index("by_conversation_role", ["conversationId", "role"]);  // ‚úÖ Compound index
```

**Step 4**: Eliminate N+1 patterns (use batch fetches)

**Step 5**: Consider denormalization for read-heavy paths
```typescript
// Instead of:
const message = await ctx.db.get(messageId);
const conversation = await ctx.db.get(message.conversationId);
const title = conversation.title;

// Denormalize:
messages: defineTable({
  conversationTitle: v.string(),  // Duplicated for speed
  // ...
});

// Trade-off: Faster reads, but must update on conversation title change
```

### Email System Best Practices

**All transactional emails MUST use React Email**:
- **Library**: `@react-email/components` + `@react-email/render`
- **Provider**: Resend (via `@convex-dev/resend` component)
- **Templates**: Store in `convex/emails/templates/` with Node runtime (`"use node"`)
- **Components**: Use shared components from `convex/emails/components/`
- **Styling**: Inline styles only (email clients don't support CSS classes)
- **Testing**: Use `testMode: true` with `delivered@resend.dev` address

**Directory structure**:
```
convex/emails/
‚îú‚îÄ‚îÄ templates/       # Individual email templates
‚îú‚îÄ‚îÄ components/      # Shared email components
‚îú‚îÄ‚îÄ utils/          # Email sending logic
‚îî‚îÄ‚îÄ test/           # Email tests
```

**Never use plain text or raw HTML strings** - always use React Email components.

---

## Codebase Reference Map

### Migration Files (Completed Phases)

**Phase 1**: Message Attachments & Tool Calls
- Migration: `/convex/migrations/001_normalize_message_attachments.ts`
- Verification: `/convex/migrations/verify_dual_write.ts`
- Cleanup: `/convex/migrations/002_cleanup_deprecated_fields.ts`
- Schema: `/convex/schema.ts` (lines 141-294, 296-309)
- Docs: `/docs/migrations/phase1-complete.md`, `/docs/migrations/phase1-testing.md`

**Phase 2**: Message Sources & Metadata
- Migration: `/convex/migrations/002_normalize_message_sources.ts`, `002_normalize_message_sources_actions.ts`
- Verification: `/convex/migrations/verify_phase2.ts`
- Operations: `/convex/sources/operations.ts`, `/convex/sources/operations_actions.ts`
- Enrichment: `/convex/sources/enrichment.ts`, `/convex/sources/enrichment_actions.ts`
- Schema: `/convex/schema.ts` (lines 310-358)
- Docs: `/docs/migrations/phase2-complete.md`, `/docs/migrations/phase2-audit-report.md`, `/docs/migrations/phase2-execution-guide.md`

### Core Tables Reference

**messages** (`/convex/schema.ts:141-294`)
- Queries: `/convex/messages.ts` (getAttachments, getToolCalls, getSources)
- Generation: `/convex/generation.ts` (streaming logic, completion)
- Frontend: `/src/components/chat/ChatMessage.tsx`

**attachments** (`/convex/schema.ts:296-309`)
- Indexes: by_message, by_conversation, by_user, by_storage
- Frontend: `/src/components/chat/AttachmentRenderer.tsx`

**toolCalls** (`/convex/schema.ts:296-309`)
- Indexes: by_message, by_conversation, by_user, by_message_partial
- Frontend: `/src/components/chat/ToolCallDisplay.tsx`

**sources** (`/convex/schema.ts:310-339`)
- Indexes: by_message, by_conversation, by_user, by_urlHash
- Frontend: `/src/components/chat/SourceList.tsx`

**sourceMetadata** (`/convex/schema.ts:341-358`)
- Indexes: by_urlHash, by_url
- Enrichment: `/convex/sources/enrichment_actions.ts`

### Key Functions Reference

**Tool Call Streaming** (`/convex/generation.ts:744-777`)
- `upsertToolCall` mutation - Real-time tool call updates
- `finalizeToolCalls` mutation - Mark partials complete

**Source Extraction** (`/convex/generation.ts:1095-1133`)
- Handles Perplexity, OpenRouter, webSearch sources
- Calls `addSources` action for URL hashing (Node runtime)

**Custom Instructions** (`/convex/users.ts:226-240`)
- ‚ö†Ô∏è Buggy mutation (loses new fields)
- Fixed in Phase 4 design with key-value store

**Cascade Delete** (`/convex/conversations.ts:282-294`)
- Current: O(P √ó C) complexity (scan all projects)
- Phase 3 design: O(1) with junction table

**Vector Search** (`/convex/search/hybrid.ts:141-181`)
- Current: Manual cosine similarity (500ms)
- Phase 7 design: Native vector index (50ms)

### TypeScript Type Depth Workarounds

**Backend locations**:
- `/convex/transcription.ts` - getCurrentUser, recordTranscription
- `/convex/search/hybrid.ts` - fullTextSearch, vectorSearch
- `/convex/ai/generateTitle.ts` - getConversationMessages
- `/convex/tts.ts` - TTS settings + type depth fixes

**Frontend locations**:
- `/src/components/settings/QuickModelSwitcher.tsx`
- `/src/components/chat/ReasoningBlock.tsx`

**Pattern**:
```typescript
// Backend: Complex cast with @ts-ignore
const result = ((await (ctx.runQuery as any)(
  // @ts-ignore - TypeScript recursion limit with 94+ Convex modules
  internal.path.to.query,
  { args },
)) as ReturnType);

// Frontend: Direct @ts-ignore
// @ts-ignore - Type depth exceeded with complex Convex mutation
const myMutation = useMutation(api.path.to.mutation);
```

---

## Performance Baselines & Targets

### Completed (Phases 1-2)

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Message size (avg) | 8KB | 4.8KB | 40% reduction |
| Migration speed | N/A | 136 msg/sec | Backfill benchmark |
| Source enrichment | N/A | 93.3% success | OpenGraph fetch rate |
| Source deduplication | 0% | 0% | Low usage (expected) |

### Designed (Phases 3-7)

| Metric | Before | After (Target) | Improvement |
|--------|--------|----------------|-------------|
| Cascade delete time | 2000ms | 200ms | 10x faster |
| Bookmark query (50 items) | 1000ms | 150ms | 7x faster |
| Vector search (1000 msgs) | 500ms | 50ms | 10x faster |
| Tag autocomplete | N/A | <100ms | New capability |
| Preference update | Full object | Single key | Atomic |

---

## Rollback & Recovery

### During Migration (Dual-Write Phase)

**If issues found**:
1. Revert code changes: `git revert <commit-sha>`
2. Deploy reverted code: `bunx convex deploy`
3. Old fields still exist ‚Üí immediate recovery
4. New tables remain but unused (can delete later)

**Data safety**:
- Old structure preserved during transition
- Dual-write ensures both locations updated
- No data loss possible

### After Cleanup (Schema Fields Removed)

**If issues found**:
1. **Cannot revert schema** (field deletion irreversible)
2. **Can rebuild from source of truth**:
   ```typescript
   // Example: Rebuild attachments array from table
   const attachments = await ctx.db
     .query("attachments")
     .withIndex("by_message", q => q.eq("messageId", msgId))
     .collect();

   await ctx.db.patch(msgId, {
     attachments: attachments.map(a => ({
       type: a.type,
       storageId: a.storageId,
       // ...
     })),
   });
   ```

**Prevention**:
- Wait 7-30 days between cleanup and schema field removal
- Monitor error logs daily during soak period
- Run verification queries before field removal

### Data Integrity Validation Queries

**Run these before proceeding to next phase**:

```typescript
// Phase 1: Verify attachments migration
export const verifyAttachments = internalQuery({
  handler: async (ctx) => {
    const messages = await ctx.db
      .query("messages")
      .filter(q => q.neq(q.field("attachments"), undefined))
      .collect();

    for (const msg of messages) {
      const newAttachments = await ctx.db
        .query("attachments")
        .withIndex("by_message", q => q.eq("messageId", msg._id))
        .collect();

      if (msg.attachments.length !== newAttachments.length) {
        console.error("Mismatch!", { msgId: msg._id, old: msg.attachments.length, new: newAttachments.length });
      }
    }
  },
});

// Phase 3: Verify project relationships
export const verifyProjectRelationships = internalQuery({
  handler: async (ctx) => {
    const projects = await ctx.db.query("projects").collect();

    for (const project of projects) {
      const links = await ctx.db
        .query("projectConversations")
        .withIndex("by_project", q => q.eq("projectId", project._id))
        .collect();

      const arrayCount = project.conversationIds?.length || 0;
      const linkCount = links.length;

      if (arrayCount !== linkCount) {
        console.error("Drift detected!", { projectId: project._id, arrayCount, linkCount });
      }
    }
  },
});
```

---

## Summary: Key Takeaways for Future Maintainers

### 1. Why We Normalized

**Not academic exercise** - solving real production problems:
- 40% bloat slowing queries
- O(N) deletes timing out
- Custom instructions mutation destroying data
- No analytics capability

### 2. Pattern That Worked: Dual-Write Migration

**6-phase approach** (Stripe pattern):
- Schema ‚Üí Backfill ‚Üí Dual-write ‚Üí Dual-read ‚Üí Cleanup ‚Üí Complete
- Zero downtime, instant rollback, gradual confidence

**Critical**: Wait 7-30 days between cleanup and schema field removal.

### 3. Technical Gotchas We Encountered

- TypeScript type depth limits (94+ modules)
- Native JSON vs stringified JSON (AI SDK v5 pattern)
- Dual source of truth drift (project arrays)
- Case-sensitive tag chaos
- Destructive object reconstruction (custom instructions)

### 4. Architecture Principles

**Always prefer**:
- Junction tables over arrays for M:N relationships
- Foreign keys over embedded IDs
- Indexes on foreign keys (by_parent pattern)
- Cursor pagination over offset
- Batch fetches over N+1 queries
- Native vector index over manual scoring
- Hash-based deduplication for shared data

**When to denormalize**:
- Read-heavy paths (e.g., `conversationTitle` in messages)
- Expensive computed values (e.g., `usageCount` in tags)
- Filtering fields in vector index (`userId` for scoping)

### 5. Migration Execution Checklist

Before starting any phase:
- [ ] Read phase doc thoroughly
- [ ] Test migration in dev environment
- [ ] Set up verification queries
- [ ] Document rollback strategy
- [ ] Schedule low-traffic deployment window

During migration:
- [ ] Deploy schema changes
- [ ] Run backfill (monitor progress)
- [ ] Verify data integrity (spot-check)
- [ ] Update queries/mutations (dual-write)
- [ ] Test critical flows
- [ ] Monitor error logs 24 hours

After migration:
- [ ] Verify performance improvements
- [ ] Check data integrity (counts match)
- [ ] User acceptance testing
- [ ] Wait 7-30 days before cleanup
- [ ] Run cleanup step
- [ ] Final verification

### 6. When to Apply These Patterns

**Normalize when**:
- Array size unbounded (e.g., conversationIds growing forever)
- Need to query relationships (e.g., "messages citing this URL")
- Duplication across entities (e.g., same tag name in 4 places)
- Analytics required (e.g., per-model token usage)

**Keep denormalized when**:
- Small, fixed-size data (e.g., user preferences if ‚â§10 fields)
- Read-only after creation (e.g., message content)
- Acceptable drift (e.g., cached counts with periodic rebuild)

### 7. Performance Optimization Priorities

**High impact, low effort**:
1. Add missing indexes (1 hour work, 10x speedup)
2. Batch fetches instead of N+1 (2 hours, 5-10x speedup)
3. Use native vector index (30 min, 10x speedup)

**Medium impact, medium effort**:
4. Normalize frequently-queried relationships (1-2 weeks, 2-5x speedup)
5. Denormalize read-heavy computed values (1 week, 2-3x speedup)

**Lower priority**:
6. Message size reduction (2 weeks, 40% smaller but queries already fast)
7. Tag deduplication (2 weeks, only impactful at scale)

### 8. Monitoring & Maintenance

**Daily** (first week after deployment):
- Check error logs (Convex dashboard)
- Monitor query performance (p95 latency)
- Run verification queries

**Weekly** (next 3 weeks):
- Data drift checks (counts, relationships)
- Performance regression checks
- User-reported issues

**Monthly** (ongoing):
- Schema health audit
- Index usage statistics
- Optimization opportunities

---

## Conclusion

This guide preserves the **why**, **how**, and **lessons learned** from blah.chat's schema normalization journey. Future phases (3-7) are fully designed and ready for implementation when needed. The patterns and gotchas documented here apply universally to any Convex database normalization effort.

**Remember**: Migrations are not just data movement - they're opportunities to fix architectural debt and enable new capabilities. Plan carefully, execute incrementally, monitor obsessively.

---

**Document Version**: 1.0
**Last Updated**: 2025-12-11
**Maintained By**: Development team (see git history for contributors)
